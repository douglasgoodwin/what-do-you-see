Machine learning & art - Google I/O 2016
Google Developers
Transcript

English - CC
0:00 DAMIEN HENRY: Hello everyone.
0:01 My name is Damien.
0:02 And I feel very lucky today, because two great artists,
0:06 Cyril Diagne and Mario Klingemann
0:09 will join me on stage in a few minutes
0:12 so you can see what they do when they use machine learning.
0:18 So if you want to go to the bathroom or text,
0:21 please do it while I'm doing the intro.
0:23 But when they are there, this is a really [INAUDIBLE].
0:27 So I'm working for the Cultural Institute in Paris.
0:31 And the mission of the Cultural Institute
0:33 is to help to a museum, an institution,
0:36 to digitalize and to shared their culture, their assets
0:42 online.
0:43 We are working with more than 1,000 museums.
0:48 And it means that if you want to discover
0:50 a new museum every week, it will take you 20 years to do so.
0:56 What we do in the Cultural Institute is this app.
0:58 It's named the Google Arts and Culture App.
1:01 It's really a beautiful app.
1:03 And if you have not downloaded yet, you should do.
1:06 There are a ton of really incredible features.
1:09 And one of my favorites is named Gigapixel.
1:13 Gigapixel is done using the art camera.
1:18 And the art camera is able to catch
1:22 every detail in a painting-- so every crack, every blue stuff,
1:26 you can see them in the app.
1:28 You can zoom in very deeply in the app, to see,
1:31 for instance, "Starry Night."
1:34 But I'm not working on this app.
1:36 I'm working in a space named The Lab.
1:38 It's in the middle of Paris.
1:39 It's a beautiful space.
1:41 I feel lucky every day when I go there.
1:44 And it's a space dedicated for creativity.
1:48 And just as a fun fact, it's where the Cardboard is born.
1:55 David Coz and I use a laser cutter
1:57 there to create the very first Cardboard-- the one that was
2:01 unveiled at I/O two years ago.
2:03 That's what I have to show today.
2:06 And last year, we also worked at the lab,
2:07 for instance, on this picture.
2:09 You can see some early prototype of the Cardboard
2:12 that was unveiled at I/O Google last year.
2:15 But here for the Cardboard today,
2:17 even if I have still a strange relationship with the VR team,
2:21 I also have a small team in Paris that's is named CILEx.
2:26 It stands for Cultural Institute Experiment Team.
2:29 And what we do, we do experiments
2:31 with creative [INAUDIBLE] and artists.
2:34 We are very passionate about three different axes.
2:39 We try to engage more people to enjoy culture.
2:43 So we try to find fun ways for people to watch more paintings.
2:48 We try to find also a new way to organize information.
2:52 So our user can have a journey in our database,
2:56 and [INAUDIBLE] can learn something out of it.
2:59 And obviously, because we have seven million assets
3:02 in the database, we try to analyze
3:03 them to discover new insights.
3:08 So this talk is about machine learning, obviously.
3:13 And just take 30 seconds to remind of the definition.
3:18 Just to make things simple, let's imagine
3:20 that you are writing an algorithm
3:22 to check if a picture is a cat picture.
3:26 You can do it yourself, by trying
3:28 to analyze all the pixels one by one, but obviously,
3:30 it's difficult. Or what you can do is, using machine learning,
3:35 having an algorithm that will learn by itself
3:39 what are the good features to check
3:42 if a picture is a cat picture.
3:48 So the key, I think, is this is happening now.
3:54 Machine learning is not the future.
3:56 Machine learning is something that everybody in this audience
3:59 can do, can try.
4:00 If you know how to code, you can try machine learning.
4:04 For example, did you know that you
4:06 can create some "Mario Brothers" levels just using a [INAUDIBLE]
4:11 networks.
4:11 Or you can make a color movie from a black and white.
4:15 Or you can make a 3D movie from a 2D movie.
4:19 So things that seem difficult or impossible
4:22 are something that you can do now using machine learning
4:25 and neural networks.
4:27 And as an example, this one is "Inside the Brother."
4:30 It's David R from Japan, designer and artist.
4:34 And he just decided to make these simple games
4:36 with two volley players.
4:38 And in fact, they play extremely well just
4:42 using a very, very simple neural network
4:44 that he displays on the screen.
4:48 So because machine learning is so useful and widespread now,
4:54 there is no doubt that it will have a huge impact
4:57 on art and on artists.
5:01 So that's why we decide something like one year
5:04 ago to create a machine learning residency in the lab.
5:09 So we asked artists to join us and to create great experience.
5:13 So now I will leave to Mario Klingemann with our latest
5:18 artist in residence.
5:20 MARIO KLINGEMANN: Thank you, Damien.
5:21 [APPLAUSE]
5:25 Hi, everybody.
5:26 My name's Mario Klingemann.
5:27 And I'm a code artists.
5:29 And might sound like I'm kind of really good at indentation
5:33 or write beautiful code.
5:35 But that's not really what it is.
5:37 It just says that I'm using code and algorithms to produce
5:42 things that look interesting.
5:43 And some of them might even be called art.
5:45 I don't know-- I'm not the one to decide that.
5:48 Like any other artist, I have this problem.
5:52 You look around you, and it looks like everything
5:55 has already been done.
5:56 I mean, in times of Google, you come up with a great idea,
6:01 you Google it, and think, oh, well, OK, done already.
6:04 So it seems there are no empty spaces anymore--
6:07 no white spaces where you can make your mark,
6:09 where you can kind of be original.
6:12 On the other hand, if you look at it,
6:16 there are no real-- humans are incapable of having
6:19 original ideas.
6:20 Ideas are always just recombination of something
6:24 some other people have done before.
6:26 You take concept A and constant B,
6:28 and the idea is finding a new connection between them.
6:32 And so this is where, for me, the computer can help
6:35 me finding these connections.
6:37 So in theory, all I have to do is
6:39 go through every possible permutation.
6:41 And the computer will offer me new combinations that,
6:44 hopefully, not have been done.
6:46 And all I have to do is sit back,
6:48 and let the whatever it has created pass by,
6:52 and decide if I like it or not.
6:54 So in a way, I'm becoming more of a curator than a creator.
6:59 I'll show you a short example.
7:00 So this is a tool I call Ernst.
7:05 It's a kind of an homage to Max Ernst, an artist
7:09 famous for his surreal collages back in the early 20th century.
7:15 And what he did, he created these collages from things
7:19 he found in papers, and catalogs, et cetera.
7:22 So I decided, well, maybe I'll build my own collage tool.
7:26 And in this case, I'm using assets
7:28 found in the vast collection of public domain images
7:32 by the Internet Archive.
7:34 And I wrote me a tool that helps me to automatically
7:37 cut them out.
7:38 And then I say, OK, if I give you these five elements, what
7:41 can you do with them?
7:43 And then it produces me stuff like these.
7:45 And unlike Max Ernst, I have the possibility
7:48 to also scale material.
7:52 And then you get these.
7:53 If you have like pipes, you get fractal structures,
7:55 things that look like plants.
7:57 And the process is very like this.
7:59 I have this tool with all the library elements,
8:01 and then it just starts combining them in random ways.
8:05 And sometimes, I see something that I like.
8:07 Very often I see things that are just horrible,
8:11 or just total chaos.
8:14 But yet, sometimes there's something that looks like this.
8:17 I call that, for example, "Run, Hipster, Run."
8:19 And I must say, coming up with funny titles
8:23 or very interesting titles is, of course, a nice perk
8:26 of this way of working.
8:28 But of course, there's still this problem
8:29 that I still have to look through a lot of images which
8:33 are just noise, just chaos.
8:36 So wouldn't it be nice if the machine could
8:39 learn what I like, what my tastes are, or, even better,
8:43 what other people like.
8:44 And then I can sell them better.
8:46 So I realized I have to first understand
8:50 what do humans find interesting in images?
8:53 What is it that makes one image more artful than another one?
8:58 And this directed my view to this growing amount
9:02 of digital archives.
9:03 And those are now-- there are lots of museums and libraries
9:07 out there that start digitizing all their old books
9:10 and paintings, just like the Cultural Institute
9:13 helps museums doing that.
9:15 And so I first stumbled upon this about two years ago,
9:18 when the British Library uploaded one million images
9:22 that were automatically extracted from books spanning
9:25 from 1500 to 1899.
9:28 There was only a little kind of a problem with it,
9:32 because all these illustrations and photos
9:35 were cut out automatically.
9:37 So they had OCR scans, and then they
9:41 knew there would be an image in a certain area.
9:44 But unless you didn't look yourself at the image,
9:47 you wouldn't know what's actually on it.
9:49 So if you were looking for, let's
9:50 say, a portrait of Shakespeare, you
9:52 would have to manually go through every image
9:54 until you maybe struck upon it.
9:56 So I thought that's a bit tricky.
9:58 Maybe I can help them with classifying their material,
10:03 and training the computer.
10:05 OK, this is a portrait.
10:06 This is a map.
10:07 So I started in a way figuring out ways how I could do that.
10:13 And eventually, I was able to tag
10:16 about 400,000 images for them.
10:18 I mean, this was a kind of group effort.
10:20 Everybody could join in.
10:22 But working with this material, I realized,
10:25 is such a joyful experience, because in the beginning,
10:28 I was just interested in the machine learning.
10:30 But actually, you suddenly realize
10:33 there's this goldmine, this huge mine of material.
10:38 And sometimes, really, you go through lots
10:41 of things that seem to be boring or you're not interested in.
10:43 And then you strike upon a beautiful illustration
10:46 or something.
10:48 And I realized that is actually a huge part
10:53 of the fun of the process.
10:54 So for example, take this rock or stone axe.
10:59 Well, once you go through this material,
11:01 you start recognizing patterns.
11:03 And so sometimes there comes this rock by.
11:05 And you say, OK, well, I don't care.
11:07 But then the second one, and you say, oh, maybe I
11:09 should start at a rock collection or rock category.
11:13 And then what happens is, suddenly
11:16 you are happy when every time you come another one of those.
11:20 And then, well, what I do is I start
11:23 arranging them, and putting them kind of in a new context.
11:27 And then you start actually starting
11:29 to appreciate the craftsmanship that went into this.
11:33 And also you can-- once you put lots of very similar things
11:36 together, you can much better distinguish
11:38 between the slight differences in there.
11:41 Yes, so I start doing this.
11:43 For example, here on the left side,
11:44 you see a piece called to 36 anonymous profiles.
11:48 There's all these 100s, 1,000s of geological profiles, which,
11:52 again, you probably, if you're not interested or are
11:55 a geologist, you wouldn't care.
11:57 But like this, it becomes a really interesting field
12:00 or just a way to bring these things that maybe sometimes
12:05 have been hidden for 100 years in a book,
12:08 and nobody has watched them.
12:10 And now you can bring them back to life.
12:12 Or on the right side, a piece I call "16 Very Sad Girls."
12:16 Again, I don't know why they have
12:18 so many sad girls in there.
12:20 But of course, again, that makes you question what
12:22 was happening at that time?
12:24 So it actually motivates you to search back
12:26 and, well, what's the story behind this?
12:29 But this all I started kind of on my own.
12:32 And this was not-- I wouldn't say it wasn't
12:35 deep learning what I was doing.
12:36 It was more classical machine learning.
12:38 Because I was always a little bit afraid
12:40 of going down this path.
12:41 I heard, oh, you need expensive machines.
12:44 And it's kind of hard to set up the machine.
12:46 So I needed something to get me motivated to go
12:51 through the painful process of installing everything to get
12:54 a machine learning system.
12:55 Luckily, about a year ago, this came along.
12:59 I don't know if you have seen this picture,
13:00 but when I saw it, thought, oh, my god.
13:03 This looks kind of weird, and I have never seen this before.
13:06 Fortunately, about a week later, after this leaked,
13:10 it was clear some engineers at Google
13:15 had come up with this new technique called Deep Dream.
13:17 And it's based on machine learning.
13:19 So of course, I wanted to know how can I do this myself?
13:22 Fortunately, what they did-- they actually
13:25 shared an IPython Notebook on GitHub,
13:27 with all the source code, even the trained model.
13:30 And it was able to dig myself into this.
13:34 Back in the days, there was no TensorFlow flow yet,
13:36 so this was still in Caffe.
13:38 But this allowed me to finally kind of go by baby steps
13:43 into learning this technique.
13:45 And obviously, I started-- like probably a lot of us--
13:49 I started to having a lot of fun with this.
13:51 You put in an image, and you wonder, oh, my god,
13:53 what I get out of this?
13:55 Because that's what it is.
13:57 You put something in.
13:58 You had no idea what you would get.
14:01 So this was fun for, let's say, a few weeks.
14:05 But then I started thinking, OK, I
14:08 think I could make some improvements to this code.
14:10 And I would call this Lucid Dreaming--
14:13 so maybe get a little bit more control or change the outcome.
14:18 So I figured out there are three points
14:21 I might be able to turn it into a different direction.
14:25 So the first one-- I'm not sure if you noticed it--
14:28 these pictures all kind of a bit psychedelic, colorful.
14:31 So I'm from Germany.
14:34 We are kind of very earnest people,
14:35 so I like it rather a bit toned down.
14:38 So very simple thing-- desaturated a bit.
14:42 And desaturation is super easy.
14:44 So all I needed was to add a single line of code, which is
14:47 the one you see at the bottom.
14:48 What you do is you take the average between the RGB values.
14:51 And then you can have a linear interpolation
14:54 between the gray-scale version and the color version.
14:57 And depending on how crazy you want it,
15:00 you pick a factor in the middle.
15:02 And so as an example here, on the left side
15:05 psychedelic Mario, on the right, the puppy face gray-scale one.
15:12 So the second thing-- that issue that you
15:20 don't know what you will get out,
15:21 or when you get something out, it
15:23 will be a selection of slug, puppy, or eye.
15:27 So can I get a little bit more control?
15:30 Well, in order to do that, we have
15:33 to have a look at how this thing works.
15:36 So this is the classic convolutional network, which
15:41 is the Google Net in this case.
15:42 That was also the architecture that
15:44 was used for the early Deep Dream experiments.
15:47 So what you do, you put something
15:49 in on the left-- an image-- and it
15:51 passes through all these convolutional layers,
15:53 and soft Max.
15:55 Well, we don't go into depth there.
15:57 In the end, you get out what the machine thinks-- a probability
16:03 for certain categories.
16:05 What Deep Dream does is, you put in an image at the beginning.
16:08 But then, instead of going all through all the network,
16:11 you stop at a certain layer.
16:13 And at that layer, there are certain neurons activated.
16:17 So depending on what the network thinks it sees there,
16:21 some neurons will get activated and others will not.
16:25 And what it does is then it emphasizes
16:27 those activations even more, and sends it back up the chain.
16:31 And then this will, in a way, kind of
16:36 emphasize all those elements.
16:38 Where it thought it has discovered, let's say, a cat.
16:41 Then it will improve the cattiness of that image.
16:48 If you look at this, what actually happens
16:50 inside this network-- and I'm coming from the kind of,
16:53 let's say, a pixel arts background,
16:55 or I'm doing lots of with bitmaps.
16:57 So I look at it this way.
16:59 Then it's actually bitmaps all the way down.
17:02 So in the top layers, you can still
17:04 see there is some convolutions going
17:06 on that reminds you of sharpening
17:08 filters or something.
17:10 The deeper you go down the net, actually, it
17:13 gets more and more abstract.
17:14 But for me, even those dots are still little bitmaps.
17:18 And then I can do stuff with them.
17:20 I can increase the brightness, reduce or enhance the contrast,
17:25 blur things.
17:27 Or what I also can do-- I can treat it
17:30 like a brain surgeon in a way.
17:32 Like I poke in, and say, OK, when I poke in here,
17:35 does the finger wiggle or the toe?
17:38 And what I can do then is, I can just
17:41 say, well, how about if I turn every neuron off
17:45 instead of, maybe, for example, the 10 most activated ones,
17:49 and send it back up?
17:50 The code to do that is, again, rather simple.
17:52 So again, each of these little bitmaps is-- well,
17:56 for me, it's a bitmap or it's a little array of numbers.
17:59 So what I can do for each of these cells,
18:01 I just sum up all the pixels in there.
18:04 Then I sort them by how much they summed up to.
18:08 And in the end, I just keep the top 10, for example-- or if I'm
18:12 just interested in a single category, the most activated
18:15 one-- and replace all the other values with 0,
18:19 and send it back up.
18:20 What I get then is something that looks like this.
18:23 So it's kind of just reducing everything
18:26 to a single category the network thinks to have seen.
18:30 And I like these, really, because they are, again,
18:32 totally out of this world.
18:34 Some remind me of organic patterns.
18:37 Sometimes you can see, OK, this might have come
18:39 from an eye detector or so.
18:42 But definitely, it doesn't contain any traces
18:46 of slugs or eyes again.
18:50 But of course, there's still this other issue.
18:52 And I call it the PuppyLeak, or rather, I kind of
18:56 reveal why are there so many puppies?
19:00 Like why does Deep Dream have such a love for puppies?
19:05 Well, the reason is that their network--
19:08 the original network-- that was used
19:11 for the first release of Deep Dream
19:13 was based on the ImageNet Large Scale Visual Recognition
19:18 Competition, which is kind of the Olympics of image
19:21 recognition.
19:22 And in 2014, what they did-- they added 150 new categories.
19:27 And it was all dog breeds.
19:30 So that old rule applies-- gerbil in, gerbil out.
19:35 So whatever you train it with, that's what you will get.
19:38 So then I thought, OK, maybe I just
19:41 have to train this network with something else.
19:45 But then kind of new to it, I heard these stories
19:49 that it takes four weeks on a super-powerful GPU
19:52 to train a network.
19:53 And I'm a poor artist.
19:56 I can't afford an NVIDIA rack with these things.
20:00 But then I came across this technique
20:02 which is really astonishing.
20:03 It's called fine tuning.
20:05 And what it does is, you can take
20:07 an already trained network, for example, the Google Net.
20:11 And all you have to do is to cut off the bottom layer.
20:15 And then you can retrain it with new material.
20:19 And what happens is, all the top layers
20:21 are pretty much the same, no matter what you train it with,
20:24 because they look for abstract elements like edges,
20:27 curvature, or things like that.
20:29 So that doesn't need to be retrained.
20:31 So doing that, you can take a trained network.
20:35 You feed in new images.
20:36 Instead of taking four weeks, you
20:38 can train a network overnight.
20:40 And the way I do it-- well, I tried it with I
20:44 called it MNIST with a twist.
20:47 I in my works with these archives,
20:50 I come across a lot of these decorative initials.
20:53 And I thought, is there a way I could actually
20:57 train it to recognize A, B, C that
21:01 come in all different kinds of shapes?
21:03 Well, I tried.
21:05 And I must admit, there is a manual process involved.
21:08 Because what I have to do is, the way I do it,
21:13 I really start folders on my hard drive,
21:15 and go manually, and drag and drop whatever I find in this.
21:20 Let's say, oh, another A. I drop in the A folder.
21:23 I don't have to do this with the thousands of images.
21:26 Actually, it turns out I can just
21:27 start with-- it's enough to take 50 for each category.
21:31 I'm pretty sure there are people who know much more than me
21:34 about machine learning.
21:35 They say, oh, my god, that will never work,
21:37 and it will totally overfit.
21:38 It doesn't matter.
21:39 Actually, it works.
21:41 So I start with just let's say 20 to 50 images per category,
21:46 and then train the network using the fine-tuning technique.
21:50 I let it kind of simmer for two hours,
21:53 so it gets a little bit accustomed to the data.
21:56 And then I use this to show me what
21:59 it thinks these letters are.
22:01 So I train it a bit.
22:04 I give it a bunch of random images.
22:07 And it says, I think it's an A. I say, no, it's not an A.
22:09 And then a B?
22:10 Oh, yes.
22:11 And actually, it gets better and better.
22:14 Because what I do is, whenever it finds something,
22:16 it gets added to my training set.
22:18 And I can repeat this process.
22:21 And in order to kind of help me with this process of saying,
22:27 yes/no, I realized that kind of left-swipe right-swipe
22:32 is a real popular way to decide if you are into a specimen
22:36 or not.
22:36 And it's actually a super-fast way.
22:40 So I can go back and forth, and in one hour
22:43 I think I can go through 1,000 images and say if it's correct
22:49 or not.
22:50 And as a result-- so for example here, this
22:53 is stuff where it has correctly recognized that it's an A.
22:57 And you can see, it's really surprising.
23:01 It comes in so many different shapes
23:03 that, well, it's just amazing how powerful
23:07 these networks are.
23:08 Then sometimes, it gives me something like this.
23:12 And I don't know.
23:14 It looks like a ruin to me.
23:16 So the machine said it's a B. And then I said, no, hmm.
23:22 So I actually went to the original scan in the book
23:25 to check it out.
23:26 And indeed, it is a B So it's really magic.
23:30 And of course, if everything you have is a hammer,
23:32 everything looks like a nail.
23:34 It start seeing letters in everything.
23:36 So it gives me these things.
23:39 But again, it's beautiful, so maybe something I
23:42 have not been looking for.
23:44 But of course, I was about Deep Dreaming.
23:46 So this is then what happens if I use this newly
23:50 trained network on material.
23:53 And you can definitely see it takes
23:54 an entirely different twist.
23:56 It suddenly has this typographic feel to it.
23:59 And another example-- not sure if you recognize the lady
24:04 on the left and the right.
24:06 But it's actually the Mona Lisa in,
24:09 let's say it has a linocut aspect to me.
24:12 But yes, there's no more puppies involved at all.
24:16 But OK, one more thing-- Deep Dream
24:21 had this great opportunity in the spring this year,
24:25 where the Grey Area Foundation located in San Francisco
24:29 was doing a charity auction to fund their projects.
24:32 And I was really honored to be invited to contribute
24:36 some artworks there.
24:37 So I kind of ate my own dog food and tried
24:41 to create something that is not obviously Deep Dream.
24:44 So I created this piece called "The Archimedes
24:46 Principle," which reminds me of a ball floating in water.
24:51 But the one thing I didn't mention yet is my residency.
24:55 And the reason is, it just started.
24:57 But I can tell you, I feel like a child in a candy store.
25:01 It's really amazing.
25:02 I have this huge amount of data to play with.
25:05 I have kind of metadata.
25:08 I have super-smart people-- much smarter
25:10 than me-- that I can ask extremely stupid questions.
25:13 And I've already started working,
25:16 but it's still something I want to fine-tune.
25:19 But I also had the privilege to see what Cyril was actually
25:23 doing there for a while.
25:24 And so, with no further ado, let's
25:27 get Cyril onstage so he can show you some awesome things.
25:32 Thank you.
25:32 [APPLAUSE]
25:33 CYRIL DIAGNE: Thank you, Mario.
25:34 Well, my name is Cyril Diagne.
25:38 And I'm a digital interaction artist.
25:41 So a question I get often is, what is a digital interaction
25:45 artist?
25:46 So it's actually very simple.
25:48 I try to create poetic moments for people
25:51 when they are interacting with digital systems.
25:54 And the process is also fairly simple.
25:57 It basically boils down to two steps.
26:00 One step is just plain, really-- just
26:04 like a kid getting my hands on some technology,
26:07 and playing without any goal, just
26:08 for the pleasure of learning new things and having fun.
26:13 And then sometimes, a poetic outcome appears.
26:16 And what that lets you do is, for example,
26:19 swing through the stars, or having your face covered
26:23 with some generative masks, or creating a video of people
26:29 dancing together, and so on, and so on,
26:32 and other things that I've been doing over the last years.
26:35 But you might be wondering, OK, what about machine learning?
26:40 It turns out one year ago, I didn't know anything
26:43 about machine learning.
26:44 It all started when Damien came to me.
26:48 He asked me, hi, Cyril, how are you?
26:51 I'm good, thanks.
26:52 How are you?
26:52 Not too bad.
26:54 And he asked me, what can you do with seven
26:58 million cultural artifacts?
27:03 I had to stop for a moment.
27:05 What do you mean seven million cultural artifacts?
27:07 What can I do?
27:08 He said, yeah, well, the Cultural Institute
27:10 has been working for several years
27:12 with thousands of partners.
27:13 We have this great database with amazing content.
27:16 Well, what can you do with it?
27:18 Well, I really had to stop, because those
27:21 are seven million opportunities to learn something new
27:25 about a culture, about some events--
27:27 really incredibly high-quality content.
27:30 So in order not to panic, I did what every coder
27:33 would do in this situation.
27:35 I [INAUDIBLE] all the assets and plotted to them in a sine wave.
27:38 Because then, I did not have to think about it anymore.
27:42 That was done.
27:43 And from there, well, I did the most obvious things.
27:48 I started to plot them by time, by color, by medium.
27:52 And, well, you find some interesting things
27:54 along the way.
27:55 But you can't help but think there's
27:57 got to be more-- all this great material, all
28:00 this great technology-- there's got to be
28:02 more that we can get out of it.
28:06 And so this is where machine learning came across.
28:10 Especially when you go to Google every day,
28:14 doesn't take too long until someone points you
28:16 to some amazing things.
28:18 And so one of the first experiments that we did
28:23 is using a popular technology now,
28:25 which is the machine learning annotation.
28:27 So it's the technology that allows
28:28 you to find the photos that you took in the forest
28:32 by typing "trees," the photos of my friend Tony, et cetera.
28:37 But what about the less expected labels-- the labels
28:40 you would not think in the first place to write down?
28:42 And what about less-expected images as well--
28:46 images that you would not expect to find anywhere there?
28:49 Well, we were curious as well.
28:52 So we sent it all over to the service.
28:55 And when we got the results back,
28:58 well, I fell off my chair.
28:59 I will show you.
29:00 Can we get onto the demo?
29:04 So basically, what we can see here
29:06 is that we got back around 4,000 different unique labels.
29:12 And we had to spend hours.
29:14 It was too amazing.
29:16 The things that were detected were really incredible.
29:18 Let's go over one example that you would expect to work,
29:22 because it does, but in quite an amazing way.
29:24 So let's look for horses.
29:27 There.
29:30 So as you can see, some beautiful artworks of horse.
29:34 Let's just take this one, for example-- beautiful painting
29:36 from Xia Xiaowan.
29:39 It's quite incredible, because I don't
29:41 know how he got his inspiration to picture horses
29:46 from this angle, because I didn't even
29:48 know it was possible, or how the machine managed
29:51 to detect that it was a horse.
29:53 But it does.
29:55 And, well, it's quite amazing.
29:58 And also, you get examples after examples of, well,
30:03 really incredible things like this,
30:05 for example, calligraphic artwork of, again, a horse.
30:08 But that really goes toward the abstract representation
30:11 from Yuan Xikun.
30:13 And again, a beautiful artwork, but it quite blows
30:17 my mind that now algorithms are capable of distinguishing
30:21 this much amount of details in these images.
30:26 So unfortunately, I can't go over all those examples.
30:31 But we are going to release this online soon.
30:34 So please take a day off to go through all of them,
30:37 because there are some pretty amazing examples.
30:39 And we said, OK, now that we realize it really works, what
30:43 about more tricky examples?
30:45 Like for example, let's put something
30:46 that leans toward emotion, like "calm," for example.
30:50 Let's have a look.
30:52 Calm-- there.
30:56 Yeah, there is this thing where we didn't know what to expect.
30:59 When you click on the label and when you look at the content,
31:02 indeed, yes, I see.
31:04 I understand your reference, computer.
31:06 These are indeed calm sceneries, beautiful landscapes-- yeah,
31:11 it's peaceful.
31:13 So as we went over and over, we came
31:17 across also labels that we would have not thought to write down.
31:20 Like for example, one that I quite
31:23 like-- I didn't know it was a thing--
31:24 but we found this "lady in waiting" collection,
31:28 automatically created, and sent back by the algorithms.
31:32 But look at this beautiful collection
31:35 of incredible artworks from various centuries of,
31:40 I don't know, maybe-- I guess it was
31:41 a thing-- "Lady in Waiting."
31:48 Oh, well, apart from that one, maybe.
31:54 And then maybe one last example-- this one I really
32:00 fell off my chair.
32:04 I had no idea what to expect.
32:06 I was, like, that must be some glitch.
32:10 But then when it appeared, well, it just makes sense.
32:14 Yeah, I mean-- that's right.
32:22 So yes we'll release this online soon.
32:26 We have to polish a few things first.
32:28 But what you can see is before the neural net is
32:33 able to label all these images, first
32:37 he is representing all the images in highly dimensional
32:44 world, let's say.
32:45 But it really qualifies as the world.
32:47 Basically, the neural net, to be able to apply those labels,
32:50 he positions every assets at a particular position
32:54 in this world.
32:55 Well, I don't know about you, but I want to visit that world.
32:59 What does it look like?
33:01 Is there an island of the Impressionists with blue hats,
33:06 or the highway of Roman statues?
33:09 I don't know, but what I suggest is you hop on with me,
33:12 and we just have a quick tour.
33:14 I will give you a quick tour.
33:16 OK, so let's start from these two assets here.
33:20 Well, they seem to have in common
33:22 that they are a landscape.
33:24 But let's take a step back.
33:27 We can see other artworks appearing with, well,
33:31 it looks like some series of landscapes-- more landscapes.
33:36 But if you look around, then we can see
33:39 that we're actually surrounded.
33:41 We literally woke up in this, I don't know, the Island of Art.
33:46 And well, let's have a tour, actually.
33:51 So here we can see a scene with animals, I guess.
34:01 As we continue, I think it's this direction.
34:05 So let's give it a little while for it to load.
34:07 People start to appear in the picture,
34:10 along with those animals.
34:13 And if we step back even more, actually,
34:18 let me take you to one of my favorite spots
34:20 directly-- this one.
34:24 I call it "The Shore of Portraits."
34:26 Look at this.
34:30 Let's give it a little while to load.
34:37 And so this-- sorry, I'm going really quick,
34:40 because we have a lot of things that I want to show.
34:42 But this is a TSNE Map.
34:46 So TSNE stands for T-distributed Stochastic Neighbor Editing,
34:51 which basically thanks all those 128 dimensions
34:55 and flattens it to just two dimensions,
34:58 so that you can plot them, and make it interactive, and easier
35:03 to travel across.
35:05 So here is a very simplistic diagram
35:08 that shows how you can create the map.
35:10 So basically, we take the image.
35:13 We feed it into the neural net which
35:14 extracts the raw features, which is a 128 dimensional vector.
35:20 Then in two steps, we reduce those dimensions to two.
35:24 And then it becomes-- you choose the way you want to plug them.
35:29 But this you can do now with eight lines of Python.
35:34 So it's not something that is reserved
35:37 for scientists or researchers.
35:40 Thanks to some amazing open source libraries like sklearn,
35:43 it is something you can do with eight lines of Python.
35:45 You just load the CSV of your raw features.
35:50 Well, I won't go over all those lines,
35:51 but you do a truncated SVD, which is the first [INAUDIBLE]
35:55 that reduces to 50 dimensions.
35:57 And then the TSNE is able to create this nice map
36:02 of two-dimensional vectors.
36:05 And then, as we saw the "Shore of Portraits,"
36:10 we got an idea, which led to what
36:14 we call the "Portrait Matcher."
36:16 So basically, the Cultural Institute--
36:19 we detected in the Art Project Channel about 40,000 faces.
36:24 So the question came quite naturally--
36:28 what if you could browse this database with your own face?
36:33 And it has to be real time, otherwise it did not happen.
36:36 So can we switch back to the demo?
36:40 All right, let's try that.
36:41 Let's see if it works.
36:45 All right, so let's see.
36:53 Oh, wow, OK.
36:54 I think we might-- OK, so we still
37:01 have to polish this one a little bit.
37:06 but if you come to our lab in Paris,
37:08 we have a much refined setup, which works much better.
37:11 But anyway, let's move on.
37:15 All right, thank you.
37:16 [APPLAUSE]
37:19 OK, can we switch back to the slides?
37:24 All right, but one good thing is that this experiment led us
37:27 to another iteration.
37:30 It came from another popular thing that
37:34 happens is when you're in front of a painting,
37:36 and you see someone drawn, and you feel like, oh, I
37:39 feel like I know this person.
37:41 Or oh, that definitely looks like my neighbor, Tony.
37:45 And actually, as it turns out, there
37:48 is this great model that's been done by some researchers
37:51 at Google which is called FaceNet.
37:55 And this model is able to achieve
37:58 99.63% accuracy on the label face
38:04 in the wild, which is the academic de facto
38:07 for this type of research.
38:10 So basically this neural net embeds in an Euclidean space
38:16 faces that are from the same identity
38:19 closer together than faces with dissimilar identities.
38:23 So basically, same people are closer in this space
38:26 than different people.
38:28 And what it sends you back, basically,
38:31 is, again, 128 dimensional vector
38:36 that represents the embedding.
38:38 And so we had to give it a try.
38:41 So who knows these guys?
38:46 OK, they're very popular in Europe, too.
38:48 So we took them as a starting point.
38:52 Let's try to find.
38:53 Let's see how well this model performs.
38:55 But let's not include in the mix other pictures of them,
38:59 because I'm sure it would work, so that would be boring.
39:01 What if, instead, we forced the system
39:04 to send us only pictures that are paintings?
39:07 Well, again, I can tell you when I saw the result,
39:11 I fell off my chair.
39:12 And yes, I did spend a lot of time on the ground
39:14 during this residency.
39:16 OK, I'm really excited to press this next button.
39:22 All right, this is what we got back.
39:26 I mean, Jared in the middle-- even though there
39:30 is a huge-- I mean, there is a gender mismatch-- that
39:33 could be his sister, right?
39:38 Or even Richard Hendricks, just on the right of him, like,
39:41 it even got the curliness of the hair.
39:44 It is just amazing.
39:45 So of course, you can imagine from there
39:47 how many tries we did.
39:50 Everyone wants to find his doppelganger in the database.
39:55 And here, who would have known that Richard Hendricks had
39:58 in half-naked man painted at the Tate Britain right now?
40:03 Let's take these guys, for example--
40:05 some of my personal heroes.
40:07 Let's see what we get.
40:09 And there, again-- even though this beautiful artwork
40:13 from Shepard Fairey for Obama campaign in 2008
40:17 is highly stylized with only four colors,
40:20 the algorithm still managed to find the matching.
40:23 And yes, that is quite amazing.
40:27 It would have not been fair not to try it with ourselves.
40:31 So we gave it a try here at I/O. And sorry, Mario,
40:35 but-- [LAUGHTER] the blue suits you so well.
40:45 So this is really fun.
40:47 And thanks again, for Damien and everyone
40:53 at the Cultural Institute for offering us
40:56 this really great opportunity.
40:57 And I will hand it over to you again.
40:59 So thank you very much.
41:01 [APPLAUSE]
41:09 DAMIEN HENRY: Thank you, Cyril.
41:11 Thank you, Mario.
41:13 I'm very happy to work with these guys.
41:15 And I'm very lucky, I guess.
41:17 So what's next?
41:19 So if you want to stay in touch with us,
41:21 please download our app.
41:23 It's a way to keep a link with you.
41:26 And our goal is to inspire you to try machine learning, too.
41:31 So if you want to give it a try, I recommend TensorFlow.
41:35 It's an open-source framework, easy to start with.
41:39 As a tutorial, the one done by Mr. Karpathy
41:43 is really, really good.
41:45 It really helps me to understand what's
41:46 going on with machine learning.
41:49 And the Cultural Institute is not the only team within Google
41:52 that is really passionate about machine learning.
41:54 So for instance, there is the MEI Initiative.
41:58 And this link is really good reading.
42:03 So I encourage you to try it.
42:05 And the last one is about the team,
42:06 named Magenta, completely dedicated to generate art
42:11 using machine learning.
42:13 So thanks, you, thanks everyone.
42:16 and that's it.
42:18 [APPLAUSE]
42:22 [MUSIC PLAYING]

Published on May 20, 2016
The recent progress of machine learning is impressive, and the applications seem endless. Neural networks are incredible tools allowing us not just to analyze but also manipulate and generate images, movies and music.

Over the last 18 months, the Lab at the Cultural Institute has invited artists and creative technologists to explore ways to apply machine learning to culture and the arts. We'll present some of our results with some onstage technical demos and a few special guests.

See all the talks from Google I/O 2016 here: https://goo.gl/olw6kV
Watch more Android talks at I/O 2016 here: https://goo.gl/Uv3jls
Watch more Chrome talks at I/O 2016 here: https://goo.gl/JoMLpB
Watch more Firebase talks at I/O 2016 here: https://goo.gl/JTH9Fr
Subscribe to the Google Developers Channel: http://goo.gl/mQyv5L

#io16 #GoogleIO #GoogleIO2016
